{
  "query": "deepseek R1",
  "collection_id": "DeepSeek_huggingface_20251221234654",
  "timestamp": "2025-12-22T00:44:50.468039",
  "results": [
    {
      "text": "3.1. DeepSeek-R1 Evaluation\nBenchmark (Metric)\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\nSonnet-1022\n0513\nV3\no1-mini o1-1217\nR1\nArchitecture\n-\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n-\n37B\n-\n-\n37B\n# Total Params\n-\n-\n671B\n-\n-\n671B\nEnglish\nMMLU (Pass@1)\n88.3\n87.2\n88.5\n85.2\n91.8\n90.8\nMMLU-Redux (EM)\n88.9\n88.0\n89.1\n86.7\n-\n92.9\nMMLU-Pro (EM)\n78.0\n72.6\n75.9\n80.3\n-\n84.0\nDROP (3-shot F1)\n88.3\n83.7\n91.6\n83.9\n90.2\n92.2\nIF-Eval (Prompt Strict)\n86.5\n84.3\n86.1\n84.8\n-\n83.3\nGPQA Diamond (Pass@1)\n65.0\n49.9\n59.1\n60.0\n75.7\n71.5\nSimpleQA (Correct)\n28.4\n38.2\n24.9\n7.0\n47.0\n30.1\nFRAMES (Acc.)\n72.5\n80.5\n73.3\n76.9\n-\n82.5\nAlpacaEval2.0 (LC-winrate)\n52.0\n51.1\n70.0\n57.8\n-\n87.6\nArenaHard (GPT-4-1106)\n85.2\n80.4\n85.5\n92.0\n-\n92.3\nCode\nLiveCodeBench (Pass@1-COT)\n38.9\n32.9\n36.2\n53.8\n63.4\n65.9\nCodeforces (Percentile)\n20.3\n23.6\n58.7\n93.4\n96.6\n96.3\nCodeforces (Rating)\n717\n759\n1134\n1820\n2061\n2029\nSWE Verified (Resolved)\n50.8\n38.8\n42.0\n41.6\n48.9\n49.2\nAider-Polyglot (Acc.)\n45.3\n16.0\n49.6\n32.9\n61.7\n53.3\nMath\nAIME 2024 (Pass@1)\n16.0\n9.3\n39.2\n63.6\n79.2\n79.8\nMATH-500 (Pass@1)\n78.3\n74.6\n90.2\n90.0\n96.4\n97.3\nCNMO 2024 (Pass@1)\n13.1\n10.8\n43.2\n67.6\n-\n78.8\nChinese\nCLUEWSC (EM)\n85.4\n87.9\n90.9\n89.9\n-\n92.8\nC-Eval (EM)\n76.7\n76.0\n86.5\n68.9\n-\n91.8\nC-SimpleQA (Correct)\n55.4\n58.7\n68.0\n40.3\n-\n63.7\nTable 4 | Comparison between DeepSeek-R1 and other representative models.\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA\nDiamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-\nprovement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\nicant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\nanalysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\ndemonstrating its capability in handling fact-based queries. A similar trend is observed where\nOpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\nanswering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\naccuracy of over 70%.\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\nmodel’s ability to follow format instructions. These improvements can be linked to the inclusion\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\nindicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\n13",
      "score": 0.5313576459884644,
      "metadata": {
        "source": "DeepSeek.pdf",
        "page": "13",
        "chunk": 13,
        "total_chunks": 22,
        "page_range": "13",
        "embedding_provider": "huggingface",
        "embedding_model": "sentence-transformers/all-mpnet-base-v2",
        "embedding_timestamp": "2025-12-21T23:45:55.832663"
      }
    },
    {
      "text": "Contents\n1\nIntroduction\n3\n1.1\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.2\nSummary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2\nApproach\n5\n2.1\nOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nDeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . .\n5\n2.2.1\nReinforcement Learning Algorithm\n. . . . . . . . . . . . . . . . . . . . . .\n5\n2.2.2\nReward Modeling\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2.3\nTraining Template\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2.4\nPerformance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\n6\n2.3\nDeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . .\n9\n2.3.1\nCold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.3.2\nReasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . .\n10\n2.3.3\nRejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . .\n10\n2.3.4\nReinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . . . .\n11\n2.4\nDistillation: Empower Small Models with Reasoning Capability . . . . . . . . . .\n11\n3\nExperiment\n11\n3.1\nDeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.2\nDistilled Model Evaluation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n4\nDiscussion\n14\n4.1\nDistillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . .\n14\n4.2\nUnsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n5\nConclusion, Limitations, and Future Work\n16\nA Contributions and Acknowledgments\n20\n2",
      "score": 0.4291587471961975,
      "metadata": {
        "source": "DeepSeek.pdf",
        "page": "2",
        "chunk": 2,
        "total_chunks": 22,
        "page_range": "2",
        "embedding_provider": "huggingface",
        "embedding_model": "sentence-transformers/all-mpnet-base-v2",
        "embedding_timestamp": "2025-12-21T23:45:26.338920"
      }
    },
    {
      "text": "Model\nAIME 2024\nMATH-500\nGPQA\nLiveCode\nCodeForces\nDiamond\nBench\npass@1\ncons@64\npass@1\npass@1\npass@1\nrating\nOpenAI-o1-mini\n63.6\n80.0\n90.0\n60.0\n53.8\n1820\nOpenAI-o1-0912\n74.4\n83.3\n94.8\n77.3\n63.4\n1843\nDeepSeek-R1-Zero\n71.0\n86.7\n95.9\n73.3\n50.0\n1444\nTable 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related\nbenchmarks.\nFigure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample\n16 responses and calculate the overall average accuracy to ensure a stable evaluation.\nDeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised\nfine-tuning data. This is a noteworthy achievement, as it underscores the model’s ability to\nlearn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-\nR1-Zero can be further augmented through the application of majority voting. For example,\nwhen majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance\nescalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The\nability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without\nmajority voting, highlights its strong foundational capabilities and its potential for further\nadvancements in reasoning tasks.\nSelf-evolution Process of DeepSeek-R1-Zero\nThe self-evolution process of DeepSeek-R1-Zero\nis a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities\nautonomously. By initiating RL directly from the base model, we can closely monitor the model’s\nprogression without the influence of the supervised fine-tuning stage. This approach provides\na clear view of how the model evolves over time, particularly in terms of its ability to handle\ncomplex reasoning tasks.\nAs depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\n7",
      "score": 0.40307000279426575,
      "metadata": {
        "source": "DeepSeek.pdf",
        "page": "7",
        "chunk": 7,
        "total_chunks": 22,
        "page_range": "7",
        "embedding_provider": "huggingface",
        "embedding_model": "sentence-transformers/all-mpnet-base-v2",
        "embedding_timestamp": "2025-12-21T23:45:39.694009"
      }
    }
  ]
}